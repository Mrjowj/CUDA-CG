/**
 * @file spmv_cuda.cu
 * @brief Professional CUDA implementation for SpMV (ELL format)
 * @author Optimized by Gemini
 */

#include "../include/kernel.h"
#include <cuda_runtime.h>
#include <iostream>
#include <cstdio>
#include <cstdlib>

// =================================================================================
// 1. Error Handling Macros
// =================================================================================

/* ========================= check macro ========================= */
#define CUDA_CHECK(call) do {                                                     \
    cudaError_t err__ = (call);                                                   \
    if (err__ != cudaSuccess) {                                                   \
        std::cerr << "CUDA error at " << __FILE__ << ":" << __LINE__              \
                  << " in " << #call << " : " << cudaGetErrorString(err__)        \
                  << std::endl;                                                   \
        std::exit(1);                                                             \
    }                                                                             \
} while (0)

#define CUDA_KERNEL_CHECK() do {                                                  \
    cudaError_t err__ = cudaGetLastError();                                       \
    if (err__ != cudaSuccess) {                                                   \
        std::cerr << "Kernel launch error at " << __FILE__ << ":" << __LINE__     \
                  << " : " << cudaGetErrorString(err__) << std::endl;             \
        std::exit(1);                                                             \
    }                                                                             \
    err__ = cudaDeviceSynchronize();                                              \
    if (err__ != cudaSuccess) {                                                   \
        std::cerr << "Kernel execution error at " << __FILE__ << ":" << __LINE__  \
                  << " : " << cudaGetErrorString(err__) << std::endl;             \
        std::exit(1);                                                             \
    }                                                                             \
} while (0)
/* ================================================================ */

// =================================================================================
// 2. Constants & Structs
// =================================================================================

constexpr int BLOCK_SIZE = 256;
#ifndef ELL_NULL
constexpr int ELL_NULL = -1;
#endif

// Context structure to hold GPU state
// 这就是 SpmvHandle 背后实际指向的对象
struct SpmvContext {
    int N;
    int ell_width;
    
    // Device Pointers
    FLOAT* d_val = nullptr;
    int* d_col = nullptr;
    FLOAT* d_x   = nullptr;
    FLOAT* d_y   = nullptr;

    // Grid Configuration
    dim3 grid;
    dim3 block;
};

// =================================================================================
// 3. CUDA Kernel (Highly Optimized)
// =================================================================================

/**
 * @brief Column-Major ELL SpMV Kernel
 * Use __ldg or const __restrict__ to force Read-Only Cache (Texture Cache) usage for d_x and d_val
 */
template <typename T>
__global__ void spmv_ell_col_major_kernel(
    const int num_rows,
    const int ell_width,
    const int* __restrict__ d_col_idx,
    const T* __restrict__ d_val,
    const T* __restrict__ d_x,
    T* __restrict__ d_y
) {
    // Global thread index
    int row = blockIdx.x * blockDim.x + threadIdx.x;

    if (row < num_rows) {
        T sum = 0.0;

        // Iterate through neighbors
        // Access Pattern: d_val[j * num_rows + row]
        // Since 'row' is consecutive for threads in a warp, memory access is COALESCED.
        for (int j = 0; j < ell_width; ++j) {
            
            // Column-Major Indexing
            int idx = j * num_rows + row;
            
            int col = d_col_idx[idx];

            if (col != ELL_NULL) {
                // Fused Multiply-Add (FMA) is automatically generated by compiler for T=float/double
                sum += d_val[idx] * d_x[col];
            }
        }
        d_y[row] = sum;
    }
}

// =================================================================================
// 4. API Implementation (Init / Exec / Free)
// =================================================================================

SpmvHandle spmv_init(int N, int ell_width, const FLOAT* h_val_cm, const int* h_col_cm) {
    // 1. Create Context
    SpmvContext* ctx = new SpmvContext();
    ctx->N = N;
    ctx->ell_width = ell_width;

    size_t size_mat_f = static_cast<size_t>(N) * ell_width * sizeof(FLOAT);
    size_t size_mat_i = static_cast<size_t>(N) * ell_width * sizeof(int);
    size_t size_vec   = static_cast<size_t>(N) * sizeof(FLOAT);

    printf("[CUDA Init] Allocating GPU memory for N=%d, Width=%d...\n", N, ell_width);

    // 2. Allocate Device Memory
    CUDA_CHECK(cudaMalloc((void**)&ctx->d_val, size_mat_f));
    CUDA_CHECK(cudaMalloc((void**)&ctx->d_col, size_mat_i));
    CUDA_CHECK(cudaMalloc((void**)&ctx->d_x,   size_vec));
    CUDA_CHECK(cudaMalloc((void**)&ctx->d_y,   size_vec));

    // 3. Transfer Static Data (Matrix A)
    // Assuming h_val_cm and h_col_cm are already in Column-Major format from the generator
    CUDA_CHECK(cudaMemcpy(ctx->d_val, h_val_cm, size_mat_f, cudaMemcpyHostToDevice));
    CUDA_CHECK(cudaMemcpy(ctx->d_col, h_col_cm, size_mat_i, cudaMemcpyHostToDevice));
    
    // Initialize x and y to zero just in case
    CUDA_CHECK(cudaMemset(ctx->d_x, 0, size_vec));
    CUDA_CHECK(cudaMemset(ctx->d_y, 0, size_vec));

    // 4. Configure Kernel
    ctx->block = dim3(BLOCK_SIZE);
    ctx->grid  = dim3((N + BLOCK_SIZE - 1) / BLOCK_SIZE);

    printf("[CUDA Init] Grid Config: %d blocks, %d threads\n", ctx->grid.x, ctx->block.x);

    // 5. Warm-up (Optional but recommended to stabilize clock speeds)
    spmv_ell_col_major_kernel<FLOAT><<<ctx->grid, ctx->block>>>(
        ctx->N, ctx->ell_width, ctx->d_col, ctx->d_val, ctx->d_x, ctx->d_y
    );
    CUDA_KERNEL_CHECK(); // Check warm-up errors

    return (SpmvHandle)ctx;
}

void spmv_exec(SpmvHandle handle, const FLOAT* h_x, FLOAT* h_y) {
    if (handle == nullptr) return;
    SpmvContext* ctx = (SpmvContext*)handle;

    size_t size_vec = static_cast<size_t>(ctx->N) * sizeof(FLOAT);

    // 1. Host -> Device (Input Vector x)
    // Note: In a full GPU solver, x would already be on GPU, avoiding this transfer.
    CUDA_CHECK(cudaMemcpy(ctx->d_x, h_x, size_vec, cudaMemcpyHostToDevice));

    // 2. Launch Kernel
    spmv_ell_col_major_kernel<FLOAT><<<ctx->grid, ctx->block>>>(
        ctx->N, ctx->ell_width, ctx->d_col, ctx->d_val, ctx->d_x, ctx->d_y
    );
    CUDA_KERNEL_CHECK();

    // 3. Device -> Host (Output Vector y)
    CUDA_CHECK(cudaMemcpy(h_y, ctx->d_y, size_vec, cudaMemcpyDeviceToHost));
}

void spmv_free(SpmvHandle handle) {
    if (handle == nullptr) return;
    SpmvContext* ctx = (SpmvContext*)handle;

    printf("[CUDA Free] Releasing GPU memory...\n");

    // Free Device Memory
    if (ctx->d_val) CUDA_CHECK(cudaFree(ctx->d_val));
    if (ctx->d_col) CUDA_CHECK(cudaFree(ctx->d_col));
    if (ctx->d_x)   CUDA_CHECK(cudaFree(ctx->d_x));
    if (ctx->d_y)   CUDA_CHECK(cudaFree(ctx->d_y));

    // Delete Context
    delete ctx;
}